# -*- coding: utf-8 -*-
"""naive_bayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ClCqZ8TCTv5viX5_XlLNGlPA49sv7sF3
"""

import numpy as np
import pandas as pd

#(Y = y): # of training instances for each class label y: 0(non-fake, Economist) or 1(fake, Onion)
#count_0
#count_1

#(Y = *): total # of training instances, irrespective of class labels: # of rows
#total_ti


#(Y = y, W = w): how many times token w appears in the documents with label y
#dict_freq[y][w]

#(Y = y, W = âˆ—): total number of tokens(all occurrences) for the documents with label y
#total_tokens_0 
#total_tokens_1


def pandas_reader(fname):
    df = pd.read_csv(fname)
    return df

    
def main():
  train = pandas_reader('/content/train.csv')
  test = pandas_reader('/content/test_noans.csv')


  #label counts
  train_labels = train["label"].to_numpy()
  train_words = train[list(train.keys())[1:-1]].to_numpy()

  features_size = len(train_words[0])

  total_ti = train.iloc[0].shape[0] - 2


  count_0 = np.count_nonzero(train_labels == 0)
  count_1 = np.count_nonzero(train_labels == 1)

  #frequencies of words in each labael in training sety

  num_words = len(train_words[0])
  dict_freq = {0: {key: 0 for key in range(num_words)}, 1: {key: 0 for key in range(num_words)}}

  w = 0
  for doc in range(0, len(train_labels)):
    if (train_labels[doc] == 0):
      for w in range(0,num_words):
        dict_freq[0][w] += train_words[doc][w]
    else:
      for w in range(0,num_words):
        dict_freq[1][w] += train_words[doc][w]
    w += 1


  total_tokens_0 = sum(dict_freq[0].values())
  total_tokens_1 = sum(dict_freq[1].values())


  #laplace smoothing
  logprobs_words_0 = np.zeros(features_size)
  logprobs_words_1 = np.zeros(features_size)

  for w in range (0,num_words-1):
    logprobs_words_0[w] = (np.log((1.0*dict_freq[0][w] + 1)/(total_tokens_0 + features_size)))
    logprobs_words_1[w] = (np.log((1.0*dict_freq[1][w] + 1)/(total_tokens_1 + features_size)))

  logprob_0 = np.log(1.0*count_0/total_ti)
  logprob_1 = np.log(1.0*count_1/total_ti)

  #TEST SET PREDICTIONS
  test_words = test[list(test.keys())[1:]].to_numpy()


  predictions = np.zeros(len(test_words), dtype = int)
  col = 0
  for doc in test_words:
    log_sums_0 = logprob_0 + np.sum(np.multiply(logprobs_words_0,doc))
    log_sums_1 = logprob_1 + np.sum(np.multiply(logprobs_words_1,doc))
    if (np.maximum(log_sums_0, log_sums_1) == log_sums_0):
      predictions[col] = 0
    else:
      predictions[col] = 1
    col += 1


  #writing into csv
  test = test.set_index("id")
  test["label"] = predictions

  test_ans = test["label"]
  test_ans.to_csv("test_ans.csv")


if __name__ == "__main__":
    main()